<0.01.4.93.16.38.16.Ava_Cruse@AVA.ADM.CS.CMU.EDU.0>
Type:     cmu.andrew.org.cmu-hci
Topic:    HCI Seminar/Wed. April 7, '93/Alex Waibel
Dates:    7-Apr-93
Time:     <stime>3:30</stime> - <etime>5:00 PM</etime>
PostedBy: Ava_Cruse on 01-Apr-93 at 16:38 from AVA.ADM.CS.CMU.EDU
Abstract: 

                                HCI SEMINAR
                        Wednesday, April 7, 1993
                              <stime>3:30</stime> - <etime>5:00pm</etime>
                              <location>Wean Hall 5409</location>
                       Multi-modal User Interfaces
                               <speaker>Alex Waibel</speaker>
                     Carnegie Mellon University
                      School of Computer Science

<paragraph><sentence>In this talk I will describe recent work aimed at developing more natural,
inobtrusive, and robust user interfaces, by including *all* the modalities
of human communication in the interpretation of human intent</sentence>.  <sentence>While
multi-media information (images, text, sound) is now becoming available
on the output side of most workstations and PC's, input is still mostly
limited to keyboard and mouse</sentence>.  <sentence>Attempts at the use of alternate modalities
have mostly focused on single input modalities and have so far found limited
acceptance</sentence>.</paragraph>

<paragraph><sentence>In an effort to improve this situation, we have recently begun to develop
ways to process a multiplicity of signals that are believed to all carry
meaning in human communication</sentence>.  <sentence>These include: Speech Understanding,
Character Recognition, Lipreading, Eye-tracking, Gesture Recognition</sentence>.  <sentence>I
will describe the signals and the methods we have developed to process them</sentence>.
<sentence>I will then look at ways in which these can be combined to derive *joint*
interpretations from multi-modal events</sentence>.  <sentence>Some modalities occur synchronously
and can (should?) be combined at the signal or recognition level (e.g.,
lipreading and speech recognition), others are asynchronous (e.g., gesture
and speech) and have to be combined at a semantic or pragmatic level</sentence>.  <sentence>I
will discuss our preliminary results in combining lipreading with speech
recognition, and gesture with speech recognition</sentence>.
<sentence>In both cases the results suggest that more robust and natural interaction
can be achieved by combination of modalities</sentence>.</paragraph>

<paragraph><sentence>I will conclude by outlining our strategy to extend these preliminary
results to practical user interfaces, such as editors and calendars</sentence>.</paragraph>
